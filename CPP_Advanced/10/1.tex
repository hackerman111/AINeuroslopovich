\chapter{Anatomy of a Thread: Cost, Kernel \& Scheduling}

Многопоточность в C++ — это не просто абстракция `std::thread`. За каждым объектом стандартной библиотеки скрывается сложный механизм операционной системы, имеющий свою цену в тактах процессора и байтах оперативной памяти. Понимание физического устройства потока, алгоритмов планировщика ядра (OS Scheduler) и накладных расходов на переключение контекста является фундаментом для построения высоконагруженных систем.

В этой главе мы деконструируем понятие "поток", разберем стоимость его создания и эксплуатации, а также проанализируем архитектурные ошибки, возникающие при наивном использовании примитивов стандарта.

\section{Физическая структура потока}

\begin{definition}{Контекст исполнения (Execution Context)}
Поток (thread) физически представляет собой совокупность состояния регистров процессора и выделенного региона памяти под стек. В многопоточной среде все потоки одного процесса разделяют общее адресное пространство (кучу, сегмент кода, глобальные переменные), но имеют изолированный стек и набор регистров.
\end{definition}

С точки зрения ядра Linux, поток — это "легковесный процесс" (LWP — Light Weight Process). Для планировщика нет принципиальной разницы между процессом и потоком, кроме того факта, что потоки делят таблицу страниц памяти (Page Table).

Минимальный набор данных, определяющий поток, включает:
\begin{enumerate}
    \item \textbf{Регистры общего назначения (GPR):} RAX, RBX, RCX и т.д. в архитектуре x86\_64.
    \item \textbf{Указатель инструкций (Instruction Pointer, RIP):} Адрес текущей исполняемой команды.
    \item \textbf{Указатель стека (Stack Pointer, RSP):} Адрес вершины стека данного потока.
    \item \textbf{Стек (Stack):} Область памяти для локальных переменных, адресов возврата и аргументов функций.
\end{enumerate}

При переключении контекста ядро ОС обязано сохранить текущее состояние регистров уходящего потока в специальную структуру в Kernel Space и загрузить значения регистров приходящего потока.

\section{Стоимость создания потока}

Создание потока — операция дорогая. В отличие от вызова функции, который занимает наносекунды (просто `call` и `push` в стек), создание потока (`std::thread`) требует системного вызова (`clone` в Linux), выделения структур ядра и настройки виртуальной памяти.

Эмпирическая оценка времени создания потока на современном железе составляет \textbf{от 5 до 50 микросекунд}. Это на несколько порядков медленнее, чем создание объекта в куче.

\subsection{Бенчмарк: Thread vs Function}

Сравним стоимость выполнения пустой работы через вызов функции и через создание нового потока.

\begin{cppcode}[]
#include <iostream>
#include <thread>
#include <vector>
#include <chrono>

void dummy_work() {
    // Имитация минимальной работы
    volatile int x = 0;
    x++;
}

int main() {
    const int N = 10000;
    
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < N; ++i) {
        dummy_work();
    }
    auto end = std::chrono::high_resolution_clock::now();
    std::cout << "Function calls: " 
              << std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() 
              << " us\n";

    start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < N; ++i) {
        std::thread t(dummy_work);
        t.join();
    }
    end = std::chrono::high_resolution_clock::now();
    std::cout << "Thread creation: " 
              << std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() 
              << " us\n";
}
\end{cppcode}

Результаты на типичном сервере показывают, что создание потоков занимает в тысячи раз больше времени. Если ваша задача выполняется быстрее, чем время создания потока (единицы микросекунд), использование `std::thread` напрямую приведет к деградации производительности.

\section{Memory Overhead: Стек и Виртуальная память}

По умолчанию в Linux размер стека для главного потока и создаваемых потоков (`pthread\_create`) составляет \textbf{8 МБ}. Это значение можно проверить командой `ulimit -s`.

\begin{important}
Поток не потребляет 8 МБ физической памяти (RAM) мгновенно. Стек выделяется в \textit{виртуальном адресном пространстве}. Физические страницы (frames) выделяются ядром лениво (Demand Paging) по мере обращения к адресам стека.
\end{important}

Однако, даже виртуальная память — ресурс исчерпаемый.
Рассмотрим задачу C10K (обслуживание 10 000 соединений). Если мы используем модель "один поток на соединение" (Thread-per-connection):

$$ 10\,000 \text{ threads} \times 8 \text{ MB} \approx 80 \text{ GB Virtual Memory} $$

Для 32-битных систем это гарантированный крах (адресное пространство всего 4 ГБ). Для 64-битных систем это создает огромную нагрузку на TLB (Translation Lookaside Buffer) процессора и структуры ядра, управляющие памятью (VMA — Virtual Memory Areas).

Если программа под нагрузкой реально использует стек (глубокая рекурсия или большие буферы на стеке), то потребление физической памяти также станет колоссальным, что приведет к OOM (Out Of Memory) Killer.

\section{Планировщик Linux (CFS)}

В современных ОС используется \textbf{вытесняющая многозадачность} (preemptive multitasking). Разработчик не контролирует, когда поток начнет исполняться и когда будет прерван. Этим занимается планировщик ядра.

В Linux используется \textbf{CFS (Completely Fair Scheduler)}.

\subsection{Механика работы CFS}
CFS моделирует "идеальный многозадачный процессор" на реальном железе.
\begin{itemize}
    \item Каждому потоку присваивается `vruntime` (virtual runtime) — время, которое поток уже провел на процессоре, взвешенное на его приоритет (nice value).
    \item Потоки организованы в \textbf{красно-черное дерево} (Red-Black Tree), отсортированное по `vruntime`.
    \item Планировщик всегда выбирает поток с наименьшим `vruntime` (самый левый узел дерева).
\end{itemize}

\subsection{Квант времени и Переключение контекста}
Минимальный интервал перепланирования (latency target) в Linux может составлять, например, 6-24 мс, но реальные кванты времени динамические.

Переключение контекста (Context Switch) — это не только сохранение регистров. Это:
\begin{enumerate}
    \item Смена режима процессора (User mode $\to$ Kernel mode).
    \item Загрязнение кэша инструкций и данных (Cache Pollution). Когда новый поток начинает исполнение, нужные ему данные, скорее всего, отсутствуют в L1/L2 кэшах.
    \item Накладные расходы на обновление структур планировщика.
\end{enumerate}

\begin{note}
Типичный сервер может иметь 256 логических ядер (например, AMD EPYC). Балансировка нагрузки и миграция потоков между NUMA-нодами в таких условиях — сложнейшая задача. Миграция потока на другое ядро аннулирует его прогретый кэш.
\end{note}

\section{Антипаттерн: Thread per Request}

Исторически веб-серверы писались по модели: "Пришел запрос $\to$ Создали поток $\to$ Обработали $\to$ Уничтожили поток".

Почему это плохо в современном HighLoad:
1.  \textbf{Unbounded Concurrency:} При всплеске трафика (DDoS или хабраэффект) создается бесконечное число потоков.
2.  \textbf{Thrashing:} Система тратит больше времени на переключение контекста между тысячами потоков, чем на полезную работу.
3.  \textbf{Latency Tail:} Время ответа начинает непредсказуемо расти из-за очередей в планировщике.

\begin{definition}{Thread Pool (Пул потоков)}
Архитектурный паттерн, при котором создается фиксированное число долгоживущих потоков (обычно равное числу аппаратных ядер), разбирающих задачи из общей очереди.
\end{definition}

Это унифицирует место создания потоков и ограничивает их максимальное число, предотвращая перегрузку системы.

\section{Проблемы стандартных абстракций C++}

Стандарт C++ предлагает высокоуровневые инструменты, которые часто вводят в заблуждение своей простотой.

\subsection{std::execution::par (C++17)}
Алгоритмы с политиками исполнения, например `std::for\_each(std::execution::par, ...)`, обещают автоматическое распараллеливание.

\begin{cppcode}[]
std::vector<int> data = ...;
std::for_each(std::execution::par, data.begin(), data.end(), [](int& x) {
    heavy_computation(x);
});
\end{cppcode}

\textbf{Проблема:} Стандарт не специфицирует, как именно реализуется параллелизм.
\begin{itemize}
    \item Реализация может создать новый поток на каждый вызов алгоритма.
    \item Если вы вызовете такой алгоритм внутри уже запущенного параллельного кода, произойдет комбинаторный взрыв числа потоков (Oversubscription).
    \item Отсутствует контроль над тем, в каком пуле исполняются задачи.
\end{itemize}

В продакшен-коде использование `std::execution::par` без глубокого понимания реализации стандартной библиотеки конкретного компилятора (libstdc++, libc++) считается небезопасным.

\subsection{std::async и std::future}
`std::async` может запускать задачу синхронно (в том же потоке) или асинхронно, в зависимости от флагов и реализации. Возвращаемый `std::future` в деструкторе блокирует поток до завершения задачи, что часто приводит к неожиданным последовательным исполнениям там, где ожидалась параллельность.

\begin{summary}
\begin{itemize}
    \item Создание потока — дорогая операция (>5 мкс). Избегайте частого создания/уничтожения потоков.
    \item Каждый поток резервирует 8 МБ виртуального адресного пространства. 100k потоков недопустимы.
    \item Переключение контекста стоит дорого из-за cache misses.
    \item Используйте Thread Pool для контроля конкурентности.
    \item Осторожно с `std::async` и `std::execution::par` — они скрывают детали управления потоками, что опасно для HighLoad.
\end{itemize}
\end{summary}
