\chapter{Аппаратные основы многопоточности и роль ОС}

Современное программирование на C++ невозможно рассматривать в отрыве от аппаратной архитектуры. Производительность кода, особенно в многопоточных приложениях, определяется не столько сложностью алгоритма, сколько эффективностью использования иерархии памяти и конвейера процессора. В этой главе мы рассмотрим эволюцию вычислительных систем от одноядерных процессоров до современных многоядерных архитектур с NUMA, а также разберем фундаментальные физические ограничения, известные как Memory Wall.

\section{Эволюция архитектуры процессоров}

\subsection{От одноядерной парадигмы к CMP}

На протяжении десятилетий доминировала парадигма последовательного исполнения кода. Программисты писали инструкции, полагая, что они будут выполнены процессором одна за другой в строгом порядке. Рост производительности обеспечивался увеличением тактовой частоты и усложнением микроархитектуры (суперскалярность, предсказание переходов).

В начале 2000-х годов этот подход достиг физического предела. Дальнейшее повышение частоты приводило к экспоненциальному росту тепловыделения и энергопотребления. Решением стал переход от наращивания частоты одного ядра к увеличению количества ядер на кристалле (Chip Multiprocessors, CMP).

\begin{definition}{SMP и CMP}
\textbf{SMP (Symmetric Multiprocessing)} — архитектура, в которой два и более одинаковых процессора подключены к общей памяти. \\
\textbf{CMP (Chip Multiprocessor)} — реализация SMP, где несколько ядер размещены на одном кристалле кремния. В 2005 году появились первые массовые многоядерные процессоры.
\end{definition}

Переход к многоядерности изменил парадигму разработки программного обеспечения. "Бесплатное" ускорение программ с выходом нового процессора прекратилось. Теперь для утилизации вычислительной мощности требуется явное распараллеливание задач.

\section{Проблема Memory Wall}

Фундаментальным ограничением производительности современных систем является диспропорция между скоростью процессора и скоростью доступа к оперативной памяти. Этот феномен получил название \textbf{Memory Wall}.

\subsection{Физические ограничения и латентность}

За последние 30 лет производительность процессоров выросла на порядки, в то время как латентность (задержка доступа) памяти осталась практически неизменной.

\begin{important}
\textbf{Ключевые показатели латентности:}
\begin{itemize}
    \item \textbf{1 такт CPU (3-4 ГГц):} $\approx 0.25 - 0.3$ нс.
    \item \textbf{Доступ в L1 Cache:} $\approx 1$ нс (3-4 такта).
    \item \textbf{Доступ в RAM:} $\approx 70 - 100$ нс.
\end{itemize}
\end{important}

Это означает, что один промах мимо кэша (cache miss) стоит процессору сотен тактов простоя. Если данные находятся в оперативной памяти, процессор вынужден ждать их получения, не выполняя полезной работы.

Физическая причина этого ограничения кроется в скорости света. Сигнал в вакууме проходит 30 см за 1 нс. В среде (медь, кремний) скорость ниже. Учитывая размеры кристалла и расстояние до модулей памяти (DIMM), сделать память одновременно большой и быстрой физически невозможно. Большая память требует длинных шин адреса и данных, что увеличивает задержку.

\subsection{Иерархия кэшей}

Для смягчения эффекта Memory Wall используется многоуровневая иерархия кэш-памяти.

\begin{itemize}
    \item \textbf{L1 Cache (Level 1):} Самый маленький (обычно 32-64 КБ), расположен непосредственно в ядре. Разделен на кэш инструкций (L1i) и кэш данных (L1d). Латентность: единицы тактов.
    \item \textbf{L2 Cache (Level 2):} Больше по объему (256 КБ - 1 МБ), чуть медленнее. Обычно является приватным для ядра.
    \item \textbf{L3 Cache (Level 3):} Большой (десятки МБ), общий для всех ядер на кристалле (Last Level Cache, LLC). Латентность: десятки тактов.
\end{itemize}

Задача кэша — хранить данные, к которым процессор обращался недавно (временная локальность) или которые находятся рядом с ними (пространственная локальность). Когда данные загружаются из памяти, они загружаются не побайтово, а целыми блоками — \textbf{кэш-линиями} (cache lines), обычно размером 64 байта.

\begin{note}
Эффективное программирование на C++ подразумевает работу с данными, которые хорошо укладываются в кэш. Последовательный доступ к массиву (vector) на порядки быстрее, чем доступ к связному списку (list), узлы которого разбросаны по куче.
\end{note}

\section{Hyper-threading (SMT)}

В попытках скрыть латентность памяти и максимально загрузить исполнительные устройства (ALU, FPU) была разработана технология SMT (Simultaneous Multithreading), наиболее известная в реализации Intel как Hyper-threading.

\subsection{Концепция виртуальных ядер}

Идея SMT заключается в том, что одно физическое ядро представляется операционной системе как два (или более) логических процессора. Ядро дублирует архитектурное состояние (регистры, счетчик команд), но исполнительные устройства (ALU, FPU, L1 кэш) остаются общими.

\begin{important}
\textbf{Hyper-threading не удваивает производительность!}
Два потока на одном ядре конкурируют за одни и те же вычислительные блоки.
\end{important}

Механизм эффективен в сценариях, когда один поток ожидает данные из памяти (memory stall). В этот момент ядро может переключиться на исполнение инструкций второго потока, загружая простаивающие ALU. Если же оба потока выполняют интенсивные вычисления (compute-bound) и не ждут памяти, выигрыш от SMT будет минимальным или даже отрицательным из-за конкуренции за кэш.

Для операционной системы и программиста это выглядит как наличие $2N$ ядер, где $N$ — число физических ядер. Например, 8 физических ядер видны как 16 логических потоков.

\section{Роль Операционной Системы}

Операционная система (ОС) выступает абстракцией над аппаратным обеспечением, предоставляя программам иллюзию монопольного владения процессором.

\subsection{Планирование и Квант Времени}

ОС использует \textbf{вытесняющую многозадачность} (preemptive multitasking). Каждому активному потоку выделяется интервал времени — \textbf{квант} (time slice), в течение которого он исполняется на процессоре.

\begin{itemize}
    \item Длительность кванта в Linux (CFS scheduler) варьируется, но порядок величины — 10-100 мс.
    \item Это значение огромно по меркам процессора ($100$ мс $= 10^8$ нс $\approx 4 \cdot 10^8$ тактов).
    \item Человеческий глаз не замечает переключений с такой частотой, создавая иллюзию параллельной работы множества приложений.
\end{itemize}

Когда квант истекает или поток блокируется (например, на вводе-выводе), происходит аппаратное прерывание таймера, и управление передается планировщику ОС.

\subsection{Переключение контекста (Context Switch)}

Процедура смены исполняемого потока называется переключением контекста.

\begin{enumerate}
    \item Сохраняется состояние текущего потока: значения регистров общего назначения (RIP, RSP, RAX и др.), флагов, регистров FPU/AVX.
    \item Выбирается следующий поток из очереди готовых к исполнению (runqueue).
    \item Восстанавливается состояние нового потока (загрузка регистров).
\end{enumerate}

Объем сохраняемых данных невелик (сотни байт), и сама операция быстрая. Однако косвенная стоимость переключения контекста может быть высокой из-за "остывания" кэша: новый поток, скорее всего, будет работать с другими данными, что приведет к серии кэш-промахов.

\subsection{Миграция потоков и Processor Affinity}

Планировщик ОС старается удерживать поток на одном и том же ядре (Processor Affinity), чтобы сохранить "прогретый" кэш (L1/L2). Миграция потока на другое ядро — дорогая операция, так как данные придется загружать в кэш нового ядра заново (из L3 или RAM).

\section{NUMA (Non-Uniform Memory Access)}

В многопроцессорных системах (серверах с несколькими сокетами) используется архитектура NUMA. Память физически разделена между процессорами.

\begin{itemize}
    \item У каждого процессора есть "своя" (локальная) память, подключенная напрямую. Доступ к ней быстрый.
    \item Доступ к памяти другого процессора (удаленная память) происходит через межпроцессорную шину (например, Intel QPI/UPI) и является более медленным.
\end{itemize}

Игнорирование NUMA-топологии в высокопроизводительных приложениях может привести к существенной деградации производительности из-за трафика по межпроцессорной шине и повышенной латентности.

\begin{summary}
\begin{itemize}
    \item Производительность упирается в память (Memory Wall). Доступ к RAM стоит сотни тактов.
    \item Иерархия кэшей критически важна. Локальность данных определяет скорость программы.
    \item Hyper-threading позволяет скрыть задержки памяти, утилизируя простаивающие такты ядра.
    \item ОС переключает потоки раз в квант времени (десятки мс), сохраняя регистровый контекст.
    \item Миграция потоков между ядрами нежелательна из-за потери горячего кэша.
\end{itemize}

\end{summary}
